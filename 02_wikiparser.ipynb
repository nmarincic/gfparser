{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp wikiparser\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-liverpool",
   "metadata": {},
   "source": [
    "# WikiParser\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.test import *\n",
    "import requests\n",
    "import time\n",
    "import chardet\n",
    "import re\n",
    "import pyprind\n",
    "import pickle\n",
    "from unicodedata import normalize\n",
    "from collections import OrderedDict\n",
    "from gfparser.word import *\n",
    "from gfparser.wordtype import *\n",
    "from gfparser.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def remove_duplicates_keep_order(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def load_words_from_file(path):\n",
    "    text = read_textfile(path)\n",
    "    text = normalize('NFC', text)\n",
    "    text = text.split('\\n')\n",
    "    mylist = []\n",
    "    for line in text:\n",
    "        f_line = __filter_line(line)\n",
    "        for word in f_line:\n",
    "            word = word.strip()\n",
    "            if (word==\"\\n\" or len(word) < 2 or word[0]==\"#\"):\n",
    "                pass\n",
    "            elif word[-1]==\"\\n\":\n",
    "                mylist.append(word[:-1])\n",
    "            else:\n",
    "                mylist.append(word)\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def __filter_line(line):\n",
    "    mylist = []\n",
    "    matches = re.findall(\"[^\\d\\W]+\", line, re.UNICODE)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            if len(match) > 1:\n",
    "                mylist.append(match)\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def read_textfile(url):\n",
    "    with open(url, \"rb\") as file:\n",
    "        rawdata = file.read()\n",
    "        guess = chardet.detect(rawdata)\n",
    "        encoding = guess['encoding']\n",
    "        confidence = guess['confidence']\n",
    "        if (float(confidence) > 0.7):\n",
    "            text = rawdata.decode(encoding)\n",
    "            return text\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def is_special(word, key, message, print=print):\n",
    "    check = re.findall(key, word.wikitext, re.UNICODE)\n",
    "    if check:\n",
    "        s = f\"'{word.title}'\"\n",
    "        print(s+message)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def is_different_spelling(word, print=print):\n",
    "    check = re.findall(r'Wortart', word.wikitext, re.UNICODE)\n",
    "    if not check:\n",
    "        print(\"No 'Wortart' attribute, maybe uncommon spelling\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def is_word_derivate(word_obj):\n",
    "    if len(word_obj.word_types)==1:\n",
    "        wtype = word_obj.word_types[0]\n",
    "        matches = re.findall(regex_basic_word, word_obj.wikitext, re.UNICODE)\n",
    "        if (matches):\n",
    "            return (True, matches[0])\n",
    "    return (False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WikiParser:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.words = []\n",
    "        self.not_found = []\n",
    "        self.sleep_len = 0.3\n",
    "        \n",
    "    def print_verbose(self, text, end='\\n'):\n",
    "        if self.verbose:\n",
    "            print (text, end=end)\n",
    "    \n",
    "    def create_parsing_url(self, word):\n",
    "        word_uri = requests.utils.quote(word)\n",
    "        url = wiki_site+wiki_options+word_uri\n",
    "        return url\n",
    "    \n",
    "    def create_normal_url(self, word):\n",
    "        url = wiki_site+wiki_normal+word\n",
    "        return url\n",
    "        \n",
    "    def fetch_wiki_data(self, word):\n",
    "        url_wiki = self.create_parsing_url(word)\n",
    "        url = self.create_normal_url(word)\n",
    "        \n",
    "        self.print_verbose (\"URL: {0}\".format(url))\n",
    "        self.print_verbose (\"URL: {0}\".format(url_wiki), end='\\n\\n')\n",
    "        \n",
    "        response = requests.get(url_wiki)\n",
    "        json_obj = response.json()\n",
    "    \n",
    "        if 'error' in json_obj:\n",
    "            self.print_verbose(f\"Word '{word}' doesn't exist in the wiktionary\")\n",
    "            return None\n",
    "        elif 'parse' in json_obj:\n",
    "            title = json_obj['parse']['title']\n",
    "            pageid = json_obj['parse']['pageid']\n",
    "            wikitext = json_obj['parse']['wikitext']['*']\n",
    "            word = Word(title, pageid, wikitext, url)\n",
    "            return word\n",
    "        \n",
    "    def clear(self):\n",
    "        self.words.clear()    \n",
    "    \n",
    "    def __fetch_words(self, wordlist):\n",
    "        valid_words = []\n",
    "        not_found = []\n",
    "        wordlist = remove_duplicates_keep_order(wordlist) # remove duplicates\n",
    "        bar = pyprind.ProgBar(len(wordlist), bar_char='█', width=50, title=\"Downloading words\")\n",
    "        for word in wordlist:\n",
    "            self.print_verbose (f'fetching the word: {word}\\n')\n",
    "            word_obj = self.fetch_wiki_data(word)\n",
    "            if word_obj:\n",
    "                word_obj = self.filter_language(word_obj, \"Deutsch\")\n",
    "                if word_obj:\n",
    "                    valid_words.append(word_obj)\n",
    "                else:\n",
    "                    self.print_verbose (f'There is not a german word named {word} on the wiktionary')\n",
    "                    not_found.append(word) # add to non-existant words\n",
    "            else:\n",
    "                not_found.append(word) # add to non-existant words\n",
    "                \n",
    "            bar.update(item_id=word+\"            \")\n",
    "            time.sleep(self.sleep_len)\n",
    "        return (valid_words, not_found)\n",
    "    \n",
    "    def filter_language(self, word, lang):\n",
    "        languages = self.extract_languages(word)\n",
    "        for key in languages:\n",
    "            if key==lang:\n",
    "                word.wikitext=languages[key]\n",
    "                return word\n",
    "        return None\n",
    "                \n",
    "    def extract_languages(self, word):\n",
    "        text = word.wikitext\n",
    "        matches = re.findall(r'(==[\\w.\\[\\]®\\<\\>\\/\\(\\)! ]*\\(\\{\\{\\w*\\|(\\w*)\\}\\}\\)[ ]*==)', text, re.UNICODE)\n",
    "        languages = {}\n",
    "        if matches:\n",
    "            for i, match in enumerate(matches):\n",
    "                self.print_verbose (\"extract_languages | language found: %s \" %match[1])\n",
    "                if (len(matches)-i > 1):\n",
    "                    lan = match[1]\n",
    "                    next_match = matches[i+1]\n",
    "                    index = text.index(match[0])\n",
    "                    next_index = text.index(next_match[0])\n",
    "                    languages[lan] = text[index:next_index]\n",
    "                else:\n",
    "                    index = text.index(match[0])\n",
    "                    languages[match[1]] = text[index:len(text)]\n",
    "            return languages\n",
    "        raise WikiParsingError(\"parsing error in 'extract languages' function\")\n",
    "       \n",
    "    def parse(self, wordlist, sleep=0.3):\n",
    "        assert isinstance(wordlist, list),\"Please pass a list of words\"\n",
    "        self.sleep_len = sleep\n",
    "        self.words, self.not_found = self.__fetch_words(wordlist)\n",
    "        self.cut_into_word_types(self.words)\n",
    "        self.fix_spelling()\n",
    "        self.fix_derivates_single()\n",
    "        self.fix_derivatives_multi()\n",
    "        self.remove_remaining_derivatives()\n",
    "        self.remove_words_without_wordtypes()\n",
    "        self.remove_main_wikitext()\n",
    "        self.parse_substantives()\n",
    "        self.parse_verbs()\n",
    "        self.parse_adjektives()\n",
    "        self.parse_IPAS()\n",
    "        self.parse_pronomens()\n",
    "        self.parse_examples()\n",
    "        \n",
    "        if self.not_found:\n",
    "            not_found_words = \", \".join(self.not_found)\n",
    "            print (f\"Didn't find words: {not_found_words}\")\n",
    "    \n",
    "    def get_wortart(self, cut):\n",
    "        matches = re.findall(regex_word_type, cut, re.UNICODE)\n",
    "        if matches:\n",
    "            name = '/'.join(matches)\n",
    "            return name\n",
    "        raise WikiParsingError(\"Uncommon error, no 'Wortart' pattern\")\n",
    "        return None\n",
    "             \n",
    "    def cut_into_word_types(self, wordlist):\n",
    "        for word_obj in wordlist:\n",
    "            indexes = []\n",
    "            matches = re.finditer(regex_wortart, word_obj.wikitext, re.MULTILINE)\n",
    "            matches_exist = sum(1 for _ in re.finditer(regex_wortart, word_obj.wikitext, re.MULTILINE))\n",
    "            if matches_exist:\n",
    "                for match in matches:\n",
    "                    indexes.append(match.start())\n",
    "                indexes.append(len(word_obj.wikitext))\n",
    "                cuts = [word_obj.wikitext[a[0]:a[1]] for a in zip(indexes[:-1],indexes[1:])]\n",
    "                for cut in cuts:\n",
    "                    excess_found = re.findall(regex_discard, cut, re.MULTILINE)\n",
    "                    if excess_found:\n",
    "                        cut_index = cut.find(excess_found[0])\n",
    "                        cut = cut[0:cut_index]\n",
    "                    w_type = self.get_wortart(cut)\n",
    "                    word_type = WordType(word_obj.title, w_type, cut)\n",
    "                    word_obj.word_types.append(word_type)\n",
    "            else:\n",
    "                self.print_verbose(f\"{word_obj} doesn't mach wortart pattern\")\n",
    "                w = WordType(word_obj.title, \"Wrong Spelling\" , word_obj.wikitext)\n",
    "                word_obj.word_types.append(w)\n",
    "\n",
    "    def fix_spelling(self):\n",
    "        to_remove = []\n",
    "        to_parse =  []\n",
    "        for word_obj in self.words:\n",
    "            if 'Wrong Spelling' in word_obj.word_types[0].name:\n",
    "                to_remove.append(word_obj)\n",
    "                wikitext = word_obj.word_types[0].wikitext\n",
    "                matches = re.findall(regex_proper_spelling, wikitext, re.UNICODE)\n",
    "                to_parse.append(matches[0])\n",
    "        if to_parse:\n",
    "            self.words = [w for w in self.words if w not in to_remove]\n",
    "            words, not_found = self.__fetch_words(to_parse)\n",
    "            self.not_found.extend(not_found)\n",
    "            self.words.extend(words)\n",
    "            self.cut_into_word_types(words)                \n",
    "    \n",
    "    def remove_remaining_derivatives(self):\n",
    "        for word_obj in self.words:\n",
    "            to_remove = []\n",
    "            w_types = word_obj.word_types\n",
    "            for w_type in w_types:\n",
    "                w_type_split = w_type.name.split('/')\n",
    "                for w in w_type_split:\n",
    "                    if w in complex_kinds:\n",
    "                        to_remove.append(w_type)\n",
    "            w_types = [w for w in w_types if w not in to_remove]\n",
    "            word_obj.word_types = w_types\n",
    "\n",
    "    def fix_derivatives_multi(self):\n",
    "        to_parse = []\n",
    "        \n",
    "        for word_obj in self.words:\n",
    "            to_remove = []\n",
    "            w_types = word_obj.word_types\n",
    "            if len(w_types) > 1:\n",
    "                for w_type in w_types:\n",
    "                    w_type_split = w_type.name.split('/')\n",
    "                    for w in w_type_split:\n",
    "                        if w in complex_kinds:\n",
    "                            matches = re.findall(regex_basic_word, w_type.wikitext, re.UNICODE)\n",
    "                            to_remove.append(w_type)\n",
    "                            if (matches):\n",
    "                                to_parse.append(matches[0])\n",
    "            w_types = [w for w in w_types if w not in to_remove]\n",
    "            word_obj.word_types = w_types\n",
    "\n",
    "        to_parse = remove_duplicates_keep_order(to_parse)\n",
    "        if to_parse:\n",
    "            words, not_found = self.__fetch_words(to_parse)\n",
    "            self.not_found.extend(not_found)\n",
    "            self.words.extend(words)\n",
    "            self.cut_into_word_types(words)  \n",
    "        \n",
    "    def remove_words_without_wordtypes(self):\n",
    "        remove = []\n",
    "        for w in self.words:\n",
    "            if not w.word_types:\n",
    "                remove.append(w)\n",
    "        self.words = [w for w in self.words if w not in remove]\n",
    "        \n",
    "    def fix_derivates_single(self):\n",
    "        to_parse, to_remove = [], []\n",
    "        \n",
    "        for word_obj in self.words:\n",
    "            is_derivate, right_word = is_word_derivate(word_obj)\n",
    "            if is_derivate:\n",
    "                to_remove.append(word_obj)\n",
    "                to_parse.append(right_word)\n",
    "        \n",
    "        to_parse = remove_duplicates_keep_order(to_parse)\n",
    "        \n",
    "        if to_parse:\n",
    "            self.words = [w for w in self.words if w not in to_remove]\n",
    "            words, not_found = self.__fetch_words(to_parse)\n",
    "            self.not_found.extend(not_found)\n",
    "            self.words.extend(words)\n",
    "            self.cut_into_word_types(words)  \n",
    "            \n",
    "    def remove_main_wikitext(self):\n",
    "        for word_obj in self.words:\n",
    "            word_obj.wikitext = [] \n",
    "    \n",
    "    def parse_substantive(self, wikitext):\n",
    "\n",
    "        def get_determiner(string):\n",
    "            if (string=='m'):\n",
    "                return 'der'\n",
    "            elif (string=='f'):\n",
    "                return 'die'\n",
    "            elif (string=='n'):\n",
    "                return 'das'\n",
    "            else:\n",
    "                print (\"Huge error in get_determiner\")\n",
    "                return None\n",
    "\n",
    "        substantive_singular = []\n",
    "        substantive_plural = []\n",
    "        has_genus = True\n",
    "        # find if there is genus at all:\n",
    "        find_wordgenus = u'\\|Genus'\n",
    "        matches = re.findall(find_wordgenus, wikitext, re.UNICODE)\n",
    "        if matches:\n",
    "            # find words with no singular:\n",
    "            find_genuszero = u'\\|Genus\\s*=0'\n",
    "            matches = re.findall(find_genuszero, wikitext, re.UNICODE)\n",
    "            if matches:\n",
    "                self.print_verbose (\"there is a 0 genus\")\n",
    "                find_genus0singular = u'Nominativ Singular\\s*=\\s*(\\w+)'\n",
    "                matches = re.findall(find_genus0singular, wikitext, re.UNICODE)\n",
    "                if matches:\n",
    "                    substantive_singular.append(matches)\n",
    "                    has_genus=False\n",
    "                else:\n",
    "                    has_genus=False\n",
    "                    substantive_singular.append('—')\n",
    "        else:\n",
    "            has_genus=False\n",
    "            self.print_verbose (\"No word genus at all!\")\n",
    "\n",
    "        if has_genus:\n",
    "            self.print_verbose ('there is genus')\n",
    "            find_genus = u'\\|Genus\\s*\\d*=(\\w)'\n",
    "            matches = re.findall(find_genus, wikitext, re.UNICODE)\n",
    "            if matches: ## there is genus\n",
    "                num_matches = len(matches)\n",
    "                self.print_verbose (f\"there are {num_matches} matches (genus):\")                \n",
    "                for i, genus in enumerate(matches):\n",
    "                    self.print_verbose (f'index: {i}, genus: {genus}')\n",
    "                    if (num_matches==1):\n",
    "                        singular = u'Nominativ Singular\\s*=\\s*(\\w+)'\n",
    "                    else: # multiple matches\n",
    "                        singular = u'Nominativ Singular\\s*'+str(i+1)+'=\\s*(\\w+)'\n",
    "                    matches = re.search(singular, wikitext, re.UNICODE)\n",
    "                    if matches:\n",
    "                        word = (matches.group(1))\n",
    "                        word = get_determiner(genus)+\" \"+word\n",
    "                        substantive_singular.append(word)\n",
    "                    else:\n",
    "                        singular = u'Nominativ Singular\\s*=\\s*(\\w+)'\n",
    "                        matches = re.search(singular, wikitext, re.UNICODE)\n",
    "                        if matches:\n",
    "                            word = (matches.group(1))\n",
    "                            word = get_determiner(genus)+\" \"+word\n",
    "                            substantive_singular.append(word)\n",
    "\n",
    "        # finding all plurals\n",
    "        find_num_plurals = u'Nominativ Plural\\s*\\d*=\\s*([\\w]+)'\n",
    "        matches = re.findall(find_num_plurals, wikitext, re.UNICODE)\n",
    "        if matches: ## there are plurals\n",
    "            num_matches = len(matches)\n",
    "            self.print_verbose (f\"there are {num_matches} matches (plural):\")\n",
    "            for i, plural in enumerate(matches):\n",
    "                self.print_verbose  (f'index: {i}, plural: {plural}')\n",
    "                if (num_matches==1):\n",
    "                    plural_search = u'Nominativ Plural\\s*=\\s*([\\w]+)'\n",
    "                else:\n",
    "                    plural_search = u'Nominativ Plural\\s*'+str(i+1)+'\\s*=\\s*([\\w]+)'\n",
    "                matches = re.search(plural_search, wikitext, re.UNICODE)\n",
    "                if matches:\n",
    "                    word = (matches.group(1))\n",
    "                    word = \"die \"+word\n",
    "                    substantive_plural.append(word)\n",
    "                else:\n",
    "                    plural_search = u'Nominativ Plural\\s*=\\s*([\\w]+)'\n",
    "                    matches = re.search(plural_search, wikitext, re.UNICODE)\n",
    "                    if matches:\n",
    "                        word = (matches.group(1))\n",
    "                        word = \"die \"+word\n",
    "                        substantive_plural.append(word)\n",
    "        else:\n",
    "            self.print_verbose (\"no plurals I guess\")\n",
    "            if (len(substantive_singular)!=0):\n",
    "                substantive_plural.append(\"—\")\n",
    "        if (len(substantive_singular)==0 and len(substantive_plural)==0):\n",
    "            self.print_verbose (\"the word is not a substantive\")\n",
    "            return None\n",
    "        return (substantive_singular, substantive_plural)\n",
    "    \n",
    "    def parse_substantives(self):\n",
    "        for word_obj in self.words:\n",
    "            for w_type in word_obj.word_types:\n",
    "                for w in w_type.name.split('/'):\n",
    "                    if w in ['Substantiv', 'Abkürzung']:\n",
    "                        res = self.parse_substantive(w_type.wikitext)\n",
    "                        if res:\n",
    "                            w_type.substantiv = res\n",
    "    \n",
    "    def parse_verb(self, wikitext):\n",
    "        praesens_ich, praeteritum_ich, partizip_ii, hilfsverb = [],[],[],[]\n",
    "\n",
    "        find_praesens_ich = u'\\|Pr\\wsens_ich\\**\\s*=([\\w ]+)'\n",
    "        matches = re.findall(find_praesens_ich, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one praesens ich\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (praesens ich):\")\n",
    "            praesens_ich = matches\n",
    "\n",
    "        find_praeteritum_ich = u'\\|Pr\\wteritum_ich\\**\\s*=\\s*([\\w ]+)'\n",
    "        matches = re.findall(find_praeteritum_ich, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one praeteritum_ich\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (praeteritum ich):\")\n",
    "            praeteritum_ich = matches\n",
    "\n",
    "\n",
    "        find_partizip_ii = u'\\|Partizip II\\**\\s*=\\s*([\\w ]+)'\n",
    "        matches = re.findall(find_partizip_ii, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one partizip_ii\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (partizip ii):\")\n",
    "            partizip_ii = matches\n",
    "\n",
    "        find_hilfsverb = u'\\|Hilfsverb\\**\\s*=\\s*([\\w ]+)'\n",
    "        matches = re.findall(find_hilfsverb, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one hilfsverb\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (hilfsverb):\")\n",
    "            hilfsverb = matches\n",
    "\n",
    "        if (len(praesens_ich)==0 and len(praeteritum_ich)==0 and len(partizip_ii)==0 and len(hilfsverb)==0):\n",
    "            self.print_verbose (\"the word is not a substantive\")\n",
    "            return None\n",
    "        return (praesens_ich, praeteritum_ich, partizip_ii, hilfsverb)  \n",
    "    \n",
    "    def parse_adjektive(self, wikitext):\n",
    "        positiv, komparativ, superlativ = \"—\",\"—\",\"—\"\n",
    "\n",
    "        find_positiv = u'\\|Positiv\\**=([\\w\\<\\>]+)'\n",
    "        matches = re.findall(find_positiv, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one positiv\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (positiv):\")\n",
    "            positiv = matches\n",
    "\n",
    "        find_komparativ = u'\\|Komparativ\\**=([\\w\\<\\>]+)'\n",
    "        matches = re.findall(find_komparativ, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one komparativ\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (komparativ):\")\n",
    "            komparativ = matches\n",
    "\n",
    "        find_superlativ = u'\\|Superlativ\\**=([\\w\\<\\>]+)'\n",
    "        matches = re.findall(find_superlativ, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one superlativ\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (superlativ):\")\n",
    "            matches = [\"am \"+m for m in matches]\n",
    "            superlativ = matches\n",
    "\n",
    "        if (positiv==\"—\" and komparativ==\"—\" and superlativ==\"—\"):\n",
    "            self.print_verbose (\"the word is not a adjektive\")\n",
    "            return None\n",
    "        return (positiv, komparativ, superlativ)\n",
    "\n",
    "    def parse_adjektives(self):\n",
    "        for word_obj in self.words:\n",
    "            for w_type in word_obj.word_types:\n",
    "                for w in w_type.name.split('/'):\n",
    "                    if w in ['Adjektiv', 'Adverb']:\n",
    "                        res = self.parse_adjektive(w_type.wikitext)\n",
    "                        if res:\n",
    "                            w_type.adjektiv = res\n",
    "\n",
    "    def parse_verbs(self):\n",
    "        for word_obj in self.words:\n",
    "            for w_type in word_obj.word_types:\n",
    "                for w in w_type.name.split('/'):\n",
    "                    if w in ['Verb']:\n",
    "                        res = self.parse_verb(w_type.wikitext)\n",
    "                        if res:\n",
    "                            w_type.verb = res\n",
    "    \n",
    "    def parse_IPA(self, wikitext):\n",
    "        IPA = []\n",
    "        find_ipa = u':{{IPA}}[\\d\\D]*?{{Lautschrift[\\d\\D]*?}}'\n",
    "        matches = re.findall(find_ipa, wikitext, re.UNICODE)\n",
    "        if not matches:\n",
    "            self.print_verbose (\"The word has no IPA!\")\n",
    "            return None\n",
    "        self.print_verbose (\"the word has an IPA!\")\n",
    "\n",
    "        find_ipa_forms = u'{{Lautschrift\\|([\\d\\D]*?)}}'\n",
    "        matches = re.findall(find_ipa_forms, wikitext, re.UNICODE)\n",
    "        if matches: #there is at least one IPA\n",
    "            self.print_verbose (f\"there are {len(matches)} matches (IPA):\")\n",
    "            #matches = [simplify_ipa(m) for m in matches] #simplify IPA\n",
    "            IPA = matches\n",
    "        if (len(IPA)==0):\n",
    "            self.print_verbose (\"The word doesn't have an IPA\")\n",
    "            return None\n",
    "        return IPA\n",
    "\n",
    "    def parse_IPAS(self):\n",
    "        for word_obj in self.words:\n",
    "            for w_type in word_obj.word_types:\n",
    "                res = self.parse_IPA(w_type.wikitext)\n",
    "                if res:\n",
    "                    w_type.IPA = res\n",
    "                    \n",
    "    def parse_pronomen(self, wikitext):\n",
    "        dic = dict()\n",
    "        counter = 0\n",
    "        l1 = ['Nominativ', 'Genitiv', 'Dativ', 'Akkusativ']\n",
    "        l2 = ['Singular', 'Plural']\n",
    "        l3 = ['m','f','n']\n",
    "        types = []\n",
    "        for a in l1:\n",
    "            for b in l2:\n",
    "                    for c in l3:\n",
    "                        if (b!=\"Plural\"):\n",
    "                            types.append(a+\" \"+b+\" \"+c)\n",
    "                        else:\n",
    "                            types.append(a+\" \"+b)\n",
    "                            break\n",
    "\n",
    "        for typ in types:\n",
    "            search = u\"\\|\"+typ+\"\\s*\\**=\\s*([\\w ]+)\"\n",
    "            matches = re.findall(search, wikitext, re.UNICODE)\n",
    "            if matches:\n",
    "                dic[typ]=matches\n",
    "            else:\n",
    "                dic[typ]=\"—\"\n",
    "                counter +=1\n",
    "        if (counter==16):\n",
    "            return None\n",
    "        return dic\n",
    "\n",
    "    def parse_pronomens(self):\n",
    "        for word_obj in self.words:\n",
    "            for w_type in word_obj.word_types:\n",
    "                for w in w_type.name.split('/'):\n",
    "                    if 'pronomen' in str.lower(w):\n",
    "                        res = self.parse_pronomen(w_type.wikitext)\n",
    "                        if res:\n",
    "                            w_type.pronomen = res           \n",
    "    \n",
    "    def cut_examples(self, wikitext):\n",
    "        find_cut = u'{{Beispiele}}([\\s\\S]+?)[= ]*[\\n]+{{[\\w ]+}}'\n",
    "        matches = re.search(find_cut, wikitext, re.UNICODE)\n",
    "        if matches:\n",
    "            return matches.group(1)\n",
    "        else:\n",
    "            find_cut = u'{{Beispiele}}([\\s\\S]+)'\n",
    "            matches = re.search(find_cut, wikitext, re.UNICODE)\n",
    "            if matches:\n",
    "                return matches.group(1)\n",
    "        raise WikiParsingError(\"Problem in cut_examples(wikitext)\")\n",
    "\n",
    "    def clean_example(self, wikitext):\n",
    "        text = wikitext\n",
    "        search_reference = u'<[Rr]ef[\\s\\S]*?>[\\s\\S]*?</[rR]ef>'\n",
    "        matches = re.findall(search_reference, wikitext, re.UNICODE)\n",
    "        if matches:\n",
    "            for m in matches:\n",
    "                text = text.replace(m,\"\")\n",
    "\n",
    "        search_reference_2 = u'\\<[Rr]ef[\\s\\S]*?\\/\\>'\n",
    "        matches = re.findall(search_reference_2, wikitext, re.UNICODE)\n",
    "        if matches:\n",
    "            for m in matches:\n",
    "                text = text.replace(m,\"\")\n",
    "\n",
    "        text = text.replace(\"''\",\"\")\n",
    "        return text\n",
    "    \n",
    "    def parse_examples_from_wordtype(self, wikitext):\n",
    "        ex = []\n",
    "        search_examples = u'{{Beispiele}}'\n",
    "        matches = re.findall(search_examples, wikitext, re.UNICODE)\n",
    "        if matches:\n",
    "            self.print_verbose (\"There are examples\")\n",
    "            cut = self.cut_examples(wikitext)\n",
    "            search_example = u'\\:\\[[\\d]+\\]\\s+(.*)'\n",
    "            matches = re.findall(search_example, cut, re.UNICODE)\n",
    "            if matches:\n",
    "                len_examples = len(matches)\n",
    "                self.print_verbose (f'number of examples: {len_examples}')\n",
    "                for example in matches:\n",
    "                    example_clean = self.clean_example(example)\n",
    "                    ex.append(example_clean)\n",
    "            return ex\n",
    "        else:\n",
    "            self.print_verbose (\"No examples\")\n",
    "            return None\n",
    "    \n",
    "    def parse_examples(self):\n",
    "        for word_obj in self.words:\n",
    "            for w_type in word_obj.word_types:\n",
    "                res = self.parse_examples_from_wordtype(w_type.wikitext)\n",
    "                if res:\n",
    "                    w_type.examples = res\n",
    "        \n",
    "    def parse3(self, wordlist, sleep=0.3):\n",
    "        self.parse_substantives()\n",
    "        self.parse_verbs()\n",
    "        self.parse_adjektives()\n",
    "        self.parse_IPAS()\n",
    "        self.parse_pronomens()\n",
    "        self.parse_examples()\n",
    "        \n",
    "        if self.not_found:\n",
    "            not_found_words = \", \".join(self.not_found)\n",
    "            print (f\"Didn't find words: {not_found_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfparser",
   "language": "python",
   "name": "gfparser"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
